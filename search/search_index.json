{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>cAdvisor (Container Advisor) provides Docker container users an understanding of the resource usage and performance characteristics of their running containers. It is a running daemon that collects, aggregates, processes, and exports information about running containers. Specifically, for each container it keeps resource isolation parameters, historical resource usage, histograms of complete historical resource usage and network statistics. This data is exported by container and machine-wide.</p> <p>Although cAdvisor has some prelimilary (useful though) UI. It also offers</p> <ol> <li>RESTful API to query container stats</li> <li>Export capability to common data storage, such as Elasticsearch</li> </ol> <p>To pull the image and run it:</p> <pre><code>sudo docker run \\\n    --volume=/:/rootfs:ro \\\n    --volume=/var/run/docker.sock:/var/run/docker.sock:rw \\\n    --volume=/sys:/sys:ro \\\n    --volume=/var/lib/docker/:/var/lib/docker:ro \\\n    --volume=/dev/disk/:/dev/disk:ro \\\n    --publish=8080:8080 \\\n    --detach=true \\\n    --name=cadvisor \\\n    --privileged \\\n    --device=/dev/kmsg \\\n    gcr.io/cadvisor/cadvisor:v0.36.0\n</code></pre> <p> </p>"},{"location":"#docker-container-stats","title":"docker-container-stats","text":"<p>cAdvisor is good for customizing container monitoring, but it's heavy. A quick-and-lightweight option would be docker-container-stats</p> <p></p>"},{"location":"api/","title":"cAdvisor Remote REST API","text":"<p>cAdvisor exposes its raw and processed stats via a versioned remote REST API:</p> <p><code>http://&lt;hostname&gt;:&lt;port&gt;/api/&lt;version&gt;/&lt;request&gt;</code></p> <p>The current version of the API is <code>v1.3</code>.</p> <p>There is a beta release of the <code>v2.0</code> API available.</p>"},{"location":"api/#version-13","title":"Version 1.3","text":"<p>This version exposes the same endpoints as <code>v1.2</code> with one additional read-only endpoint.</p>"},{"location":"api/#events","title":"Events","text":"<p>The resource name for Docker container information is as follows:</p> <p><code>/api/v1.3/events/&lt;absolute container name&gt;</code></p> <p>Querying the endpoint receives a list of events which are a serialized <code>Event</code> JSON objects (found in info/v1/container.go).</p> <p>The endpoint accepts a certain number of query parameters:</p> Parameter Description Default <code>start_time</code> Start time of events to query (for stream=false) Beginning of time <code>end_time</code> End time of events to query (for stream=false) Now <code>stream</code> Whether to stream new events as they occur. If false returns historical events false <code>subcontainers</code> Whether to also return events for all subcontainers false <code>max_events</code> The max number of events to return (for stream=false) 10 <code>all_events</code> Whether to include all supported event types false <code>oom_events</code> Whether to include OOM events false <code>oom_kill_events</code> Whether to include OOM kill events false <code>creation_events</code> Whether to include container creation events false <code>deletion_events</code> Whether to include container deletion events false"},{"location":"api/#version-12","title":"Version 1.2","text":"<p>This version exposes the same endpoints as <code>v1.1</code> with one additional read-only endpoint.</p>"},{"location":"api/#docker-container-information","title":"Docker Container Information","text":"<p>The resource name for Docker container information is as follows:</p> <p><code>/api/v1.2/docker/&lt;Docker container name or blank for all Docker containers&gt;</code></p> <p>The Docker name can be either the UUID or the short name of the container. It returns the information of the specified container(s). The information is returned as a list of serialized <code>ContainerInfo</code> JSON objects (found in info/v1/container.go).</p>"},{"location":"api/#version-11","title":"Version 1.1","text":"<p>This version exposes the same endpoints as <code>v1.0</code> with one additional read-only endpoint.</p>"},{"location":"api/#subcontainer-information","title":"Subcontainer Information","text":"<p>The resource name for subcontainer information is as follows:</p> <p><code>/api/v1.1/subcontainers/&lt;absolute container name&gt;</code></p> <p>Where the absolute container name follows the lmctfy naming convention (described bellow). It returns the information of the specified container and all subcontainers (recursively). The information is returned as a list of serialized <code>ContainerInfo</code> JSON objects (found in info/v1/container.go).</p>"},{"location":"api/#version-10","title":"Version 1.0","text":"<p>This version exposes two main endpoints, one for container information and the other for machine information. Both endpoints are read-only in v1.0.</p>"},{"location":"api/#container-information","title":"Container Information","text":"<p>The resource name for container information is as follows:</p> <p><code>/api/v1.0/containers/&lt;absolute container name&gt;</code></p> <p>Where the absolute container name follows the lmctfy naming convention. For example:</p> Container Name Resource Name / /api/v1.0/containers/ /foo /api/v1.0/containers/foo /docker/2c4dee605d22 /api/v1.0/containers/docker/2c4dee605d22 <p>Note that the root container (<code>/</code>) contains usage for the entire machine. All Docker containers are listed under <code>/docker</code>.</p> <p>The container information is returned as a JSON object containing:</p> <ul> <li>Absolute container name</li> <li>List of subcontainers</li> <li>ContainerSpec which describes the resource isolation enabled in the container</li> <li>Detailed resource usage statistics of the container for the last <code>N</code> seconds (<code>N</code> is globally configurable in cAdvisor)</li> <li>Histogram of resource usage from the creation of the container</li> </ul> <p>The actual object is the marshalled JSON of the <code>ContainerInfo</code> struct found in info/v1/container.go</p>"},{"location":"api/#machine-information","title":"Machine Information","text":"<p>The resource name for machine information is as follows:</p> <p><code>/api/vX.Y/machine</code></p> <p>This resource is read-only. The machine information is returned as a JSON object containing:</p> <ul> <li>Number of schedulable logical CPU cores</li> <li>Memory capacity (in bytes)</li> <li>Maximum supported CPU frequency (in kHz)</li> <li>Available filesystems: major, minor numbers and capacity (in bytes)</li> <li>Network devices: mac addresses, MTU, and speed (if available)</li> <li>Machine topology: Nodes, cores, threads, per-node memory, and caches</li> </ul> <p>The actual object is the marshalled JSON of the <code>MachineInfo</code> struct found in info/v1/machine.go</p>"},{"location":"api_v2/","title":"cAdvisor Remote REST API","text":"<p>cAdvisor exposes its raw and processed stats via a versioned remote REST API:</p> <p><code>http://&lt;hostname&gt;:&lt;port&gt;/api/&lt;version&gt;/&lt;request&gt;</code></p> <p>This document covers the detail of version 2.0. All resources covered in this version are read-only.</p> <p>NOTE: v2.0 is still a work in progress.</p>"},{"location":"api_v2/#version-information","title":"Version information","text":"<p>Software version for cAdvisor can be obtained from version endpoint as follows: <code>/api/v2.0/version</code></p>"},{"location":"api_v2/#machine-information","title":"Machine Information","text":"<p>The resource name for machine information is as follows:</p> <p><code>/api/v2.0/machine</code></p> <p>The machine information is returned as a JSON object of the <code>MachineInfo</code> struct found in info/v1/machine.go</p>"},{"location":"api_v2/#attributes","title":"Attributes","text":"<p>Attributes endpoint provides hardware and software attributes of the running machine. The resource name for attributes is: <code>/api/v2.0/attributes</code></p> <p>Hardware information includes all information covered by machine endpoint. Software information include version of cAdvisor, kernel, docker, and underlying OS.</p> <p>The actual object is the marshalled JSON of the <code>Attributes</code> struct found in info/v2/machine.go</p>"},{"location":"api_v2/#container-stats","title":"Container Stats","text":"<p>The resource name for container stats information is: <code>/api/v2.0/stats/&lt;container identifier&gt;</code></p>"},{"location":"api_v2/#stats-request-options","title":"Stats request options","text":"<p>Stats support following options in the request: - <code>type</code>: describes the type of identifier. Supported values are <code>name</code>(default) and <code>docker</code>. <code>name</code> implies that the identifier is an absolute container name. <code>docker</code> implies that the identifier is a docker id. - <code>recursive</code>: Option to specify if stats for subcontainers of the requested containers should also be reported. Default is false. - <code>count</code>: Number of stats samples to be reported. Default is 64.</p>"},{"location":"api_v2/#container-name","title":"Container name","text":"<p>When container identifier is of type <code>name</code>, the identifier is interpreted as the absolute container name. Naming follows the lmctfy convention. For example:</p> Container Name Resource Name / /api/v2.0/containers/ /foo /api/v2.0/containers/foo /docker/2c4dee605d22 /api/v2.0/containers/docker/2c4dee605d22 <p>Note that the root container (<code>/</code>) contains usage for the entire machine. All Docker containers are listed under <code>/docker</code>. Also, <code>type=name</code> is not required in the examples above as <code>name</code> is the default type.</p>"},{"location":"api_v2/#docker-containers","title":"Docker Containers","text":"<p>When container identifier is of type <code>docker</code>, the identifier is interpreted as docker id. For example:</p> Docker container Resource Name All docker containers /api/v2.0/stats?type=docker&amp;recursive=true clever_colden /api/v2.0/stats/clever_colden?type=docker 2c4dee605d22 /api/v2.0/stats/2c4dee605d22?type=docker <p>The Docker name can be either the UUID or the short name of the container. It returns the information of the specified container(s).</p> <p>Note that <code>recursive</code> is only valid when docker root is specified. It is used to get stats for all docker containers.</p>"},{"location":"api_v2/#returned-stats","title":"Returned stats","text":"<p>The stats information is returned  as a JSON object containing a map from container name to list of stat objects. Stat object is the marshalled JSON of the <code>ContainerStats</code> struct found in info/v2/container.go</p>"},{"location":"api_v2/#container-stats-summary","title":"Container Stats Summary","text":"<p>Instead of a list of periodically collected detailed samples, cAdvisor can also provide a summary of stats for a container. It provides the latest collected stats and percentiles (max, average, and 90%ile) values for usage in last minute and hour. (Usage summary for last day exists, but is not currently used.)</p> <p>Unlike the regular stats API, only selected resources are captured by <code>summary</code>. Currently it is limited to cpu and memory usage.</p> <p>The resource name for container summary information is: <code>/api/v2.0/summary/&lt;container identifier&gt;</code></p> <p>Additionally, <code>type</code> and <code>recursive</code> options can be used to describe the identifier type and ask for summary of all subcontainers respectively. The semantics are same as described for container stats above.</p> <p>The returned summary information is a JSON object containing a map from container name to list of summary objects. Summary object is the marshalled JSON of the <code>DerivedStats</code> struct found in info/v2/container.go</p>"},{"location":"api_v2/#container-spec","title":"Container Spec","text":"<p>The resource name for container stats information is: <code>/api/v2.0/spec/&lt;container identifier&gt;</code></p> <p>Additionally, <code>type</code> and <code>recursive</code> options can be used to describe the identifier type and ask for spec of all subcontainers respectively. The semantics are same as described for container stats above.</p> <p>The spec information is returned as a JSON object containing a map from container name to list of spec objects. Spec object is the marshalled JSON of the <code>ContainerSpec</code> struct found in info/v2/container.go</p>"},{"location":"application_metrics/","title":"Collecting Application Metrics with cAdvisor","text":"<p>Note Application metrics support is in Alpha. We are still making a bunch of interface changes.</p>"},{"location":"application_metrics/#introduction","title":"Introduction","text":"<p>In addition to usage metrics, cAdvisor can also be configured to collect application metrics. A container can expose application metrics through multiple ways - on a status page, through structured info like prometheus, or have a separate API for fetching stats. cAdvisor provides a generic way to collect these metrics. Additional templates are provided to automate some well-known collection profiles.</p>"},{"location":"application_metrics/#specifying-application-metrics","title":"Specifying application metrics","text":"<p>Application metrics specification consists of two steps: * Creating a configuration * Passing the configuration location to cadvisor</p>"},{"location":"application_metrics/#creating-a-configuration","title":"Creating a configuration","text":"<p>An application metric configuration tells cAdvisor where to look for application metrics and specifies other parameters about how to export the metrics from cAdvisor to UI and backends. The metric config includes: * Endpoint (Location to collect metrics from) * Name of metric * Type (Counter, Gauge, ...) * Data Type (int, float) * Units (kbps, seconds, count) * Polling Frequency * Regexps (Regular expressions to specify which metrics to collect and how to parse them)</p> <p>Here is an example of a very generic metric collector that assumes no structured information:</p> <pre><code>{\n  \"endpoint\" : \"http://localhost:8000/nginx_status\",\n  \"metrics_config\" : [\n    {\n      \"name\" : \"activeConnections\",\n      \"metric_type\" : \"gauge\",\n      \"units\" : \"number of active connections\",\n      \"data_type\" : \"int\",\n      \"polling_frequency\" : 10,\n      \"regex\" : \"Active connections: ([0-9]+)\"\n    },\n    {\n      \"name\" : \"reading\",\n      \"metric_type\" : \"gauge\",\n      \"units\" : \"number of reading connections\",\n      \"data_type\" : \"int\",\n      \"polling_frequency\" : 10,\n      \"regex\" : \"Reading: ([0-9]+) .*\"\n    }\n  ]\n} \n</code></pre> <p>For structured metrics export, eg. Prometheus, the config can shrink down to just the endpoint, as other information can be gleaned from the structure. Here is a sample prometheus config that collects all metrics from an endpoint.</p> <pre><code>{\n  \"endpoint\" : \"http://localhost:9100/metrics\"\n}\n</code></pre> <p>Another sample config that collects only selected metrics:</p> <pre><code>{\n  \"endpoint\" : \"http://localhost:8000/metrics\",\n  \"metrics_config\" : [\n    \"scheduler_binding_latency\",\n    \"scheduler_e2e_scheduling_latency\",\n    \"scheduling_algorithm_latency\"\n  ]\n}\n</code></pre>"},{"location":"application_metrics/#passing-the-configuration-to-cadvisor","title":"Passing the configuration to cAdvisor","text":"<p>cAdvisor can discover any configurations for a container using Docker container labels. Any label starting with <code>io.cadvisor.metric</code> is parsed as a cadvisor application-metric label. cAdvisor uses the value as an indicator of where the configuration can be found.  Labels of the form <code>io.cadvisor.metric.prometheus-xyz</code> indicate that the configuration points to a Prometheus metrics endpoint.</p> <p>The configuration file can either be part of the container image or can be added on at runtime with a volume. This makes sure that there is no connection between the host where the container is running and the application metrics configuration. A container is self-contained for its metric information.</p> <p>So a sample configuration for redis would look like:</p> <p>Dockerfile (or runtime): <pre><code> FROM redis\n ADD redis_config.json /var/cadvisor/redis_config.json\n LABEL io.cadvisor.metric.redis=\"/var/cadvisor/redis_config.json\"\n</code></pre></p> <p>cAdvisor will then reach into the container image at runtime, process the config, and start collecting and exposing application metrics.</p> <p>Note that cAdvisor specifically looks at the container labels to extract this information.  In Docker 1.8, containers don't inherit labels from their images, and thus you must specify the label at runtime.</p>"},{"location":"application_metrics/#api-access-to-application-specific-metrics","title":"API access to application-specific metrics","text":"<p>A new endpoint is added for collecting application-specific metrics for a particular container:</p> <pre><code>http://localhost:8080/api/v2.0/appmetrics/containerName\n</code></pre> <p>The set of application-metrics being collected can be discovered from the container spec:</p> <pre><code>http://localhost:8080/api/v2.0/spec/containerName\n</code></pre> <p>Regular stats API also has application-metrics appended to it:</p> <pre><code>http://localhost:8080/api/v2.0/stats/containerName\n</code></pre>"},{"location":"application_metrics/#ui-changes","title":"UI changes","text":"<p>Application-metrics show up on the container page after the resource metrics.</p>"},{"location":"application_metrics/#ongoing-work","title":"Ongoing work","text":""},{"location":"application_metrics/#templates","title":"Templates","text":"<p>Next step for application-metrics is to add templates for well-known containers that have stable stats API. These would be specified by a new label <code>io.cadvisor.metric.type</code>. If the label value is a known type, cAdvisor would start collecting stats automatically without needing any further config. Config can still be used to override any specific parameters - like set of metrics to collect. </p>"},{"location":"application_metrics/#ui-enhancements","title":"UI enhancements","text":"<p>There are a bunch of UI enhancements under way: * Better handling/display of metrics - eg. allowing overlaying metrics on the same graphs, handling metric types like percentiles. * Moving application metrics to separate tab. * Adding control to show only selected metrics on UI while still exporting everything through the API.</p>"},{"location":"clients/","title":"cAdvisor API Clients","text":"<p>There is an official Go client implementation in the client directory. You can use it on your own Go project by including it like this:</p> <pre><code>import \"github.com/google/cadvisor/client\"\n\nclient, err = client.NewClient(\"http://localhost:8080/\")\nmInfo, err := client.MachineInfo()\n</code></pre> <p>Do you know of another cAdvisor client? Maybe in another language? Please let us know! We'd be happy to add a note on this page.</p>"},{"location":"deploy/","title":"Building and Deploying the cAdvisor Docker Container","text":""},{"location":"deploy/#building","title":"Building","text":"<p>Building the cAdvisor Docker container is simple, just run:</p> <pre><code>$ ./deploy/build.sh\n</code></pre> <p>Which will statically build the cAdvisor binary and then build the Docker image. The resulting Docker image will be called <code>google/cadvisor:beta</code>. This image is very bare, containing the cAdvisor binary and nothing else.</p>"},{"location":"deploy/#deploying","title":"Deploying","text":"<p>All cAdvisor releases are tagged and correspond to a Docker image. The latest supported release uses the <code>latest</code> tag. We have a <code>beta</code> and <code>canary</code> tag for pre-release versions with newer features. You can see more details about this in the cAdvisor Google Container Registry page.</p>"},{"location":"running/","title":"Running cAdvisor","text":""},{"location":"running/#with-docker","title":"With Docker","text":"<p>We have a Docker image that includes everything you need to get started. Simply run:</p> <pre><code>VERSION=v0.35.0 # use the latest release version from https://github.com/google/cadvisor/releases\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  gcr.io/cadvisor/cadvisor:$VERSION\n</code></pre> <p>cAdvisor is now running (in the background) on <code>http://localhost:8080/</code>. The setup includes directories with Docker state cAdvisor needs to observe.</p> <p>Note: - If docker daemon is running with user namespace enabled, you need to add <code>--userns=host</code> option in order for cAdvisor to monitor Docker containers, otherwise cAdvisor can not connect to docker daemon. - If cadvisor scrapes <code>process</code> metrics due to <code>--disable_metrics</code> or <code>--enable_metrics</code> options, you need to add <code>--pid=host</code> and <code>--privileged</code> for <code>docker run</code> to get <code>/proc/pid/fd</code> path in host. - If cAdvisor needs to be run in Docker container without <code>--privileged</code> option it is possible to add host devices to container using <code>--dev</code> and   specify security options using <code>--security-opt</code> with secure computing mode (seccomp).   For details related to seccomp please see, the default Docker profile can be found here.</p> <p>For example to run cAdvisor with perf support in Docker container without <code>--privileged</code> option it is required to:   - Set perf_event_paranoid using <code>sudo sysctl kernel.perf_event_paranoid=-1</code>, see documentation   - Add \"perf_event_open\" syscall into syscalls array with the action: \"SCMP_ACT_ALLOW\" in default Docker profile   - Run Docker container with following options:   <pre><code>docker run \\\n--volume=/:/rootfs:ro \\\n--volume=/var/run:/var/run:ro \\\n--volume=/sys:/sys:ro \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\n--volume=/dev/disk/:/dev/disk:ro \\\n--volume=$GOPATH/src/github.com/google/cadvisor/perf/testing:/etc/configs/perf \\\n--publish=8080:8080 \\\n--device=/dev/kmsg \\\n--security-opt seccomp=default.json \\\n--name=cadvisor \\\ngcr.io/cadvisor/cadvisor:&lt;tag&gt; -perf_events_config=/etc/configs/perf/perf.json\n</code></pre></p>"},{"location":"running/#with-boot2docker","title":"With Boot2Docker","text":"<p>After booting up a boot2docker instance, run cAdvisor image with the same docker command mentioned above. cAdvisor can now be accessed at port 8080 of your boot2docker instance. The host IP can be found through DOCKER_HOST environment variable setup by boot2docker:</p> <pre><code>$ echo $DOCKER_HOST\ntcp://192.168.59.103:2376\n</code></pre> <p>In this case, cAdvisor UI should be accessible on <code>http://192.168.59.103:8080</code></p>"},{"location":"running/#other-configurations","title":"Other Configurations","text":""},{"location":"running/#centos-fedora-and-rhel","title":"CentOS, Fedora, and RHEL","text":"<p>You may need to run the container with <code>--privileged=true</code> and <code>--volume=/cgroup:/cgroup:ro \\</code> in order for cAdvisor to monitor Docker containers.</p> <p>RHEL and CentOS lock down their containers a bit more. cAdvisor needs access to the Docker daemon through its socket. This requires <code>--privileged=true</code> in RHEL and CentOS.</p> <p>On some versions of RHEL and CentOS the cgroup hierarchies are mounted in <code>/cgroup</code> so run cAdvisor with an additional Docker option of <code>--volume=/cgroup:/cgroup:ro \\</code>.</p> <p>Note: For a RedHat 7 docker host the default run commands from above throw oci errors. Please use the command below if the host is RedHat 7: <pre><code>docker run\n--volume=/:/rootfs:ro\n--volume=/var/run:/var/run:rw\n--volume=/sys/fs/cgroup/cpu,cpuacct:/sys/fs/cgroup/cpuacct,cpu\n--volume=/var/lib/docker/:/var/lib/docker:ro\n--publish=8080:8080\n--detach=true\n--name=cadvisor\n--privileged=true\ngoogle/cadvisor:latest\n</code></pre></p>"},{"location":"running/#debian","title":"Debian","text":"<p>By default, Debian disables the memory cgroup which does not allow cAdvisor to gather memory stats. To enable the memory cgroup take a look at these instructions.</p>"},{"location":"running/#lxc-docker-exec-driver","title":"LXC Docker exec driver","text":"<p>If you are using Docker with the LXC exec driver, then you need to manually specify all cgroup mounts by adding the:</p> <pre><code>  --volume=/cgroup/cpu:/cgroup/cpu \\\n  --volume=/cgroup/cpuacct:/cgroup/cpuacct \\\n  --volume=/cgroup/cpuset:/cgroup/cpuset \\\n  --volume=/cgroup/memory:/cgroup/memory \\\n  --volume=/cgroup/blkio:/cgroup/blkio \\\n</code></pre>"},{"location":"running/#invalid-bindmount","title":"Invalid Bindmount <code>/</code>","text":"<p>This is a problem seen in older versions of Docker. To fix, start cAdvisor without the <code>--volume=/:/rootfs:ro</code> mount. cAdvisor will degrade gracefully by dropping stats that depend on access to the machine root.</p>"},{"location":"running/#standalone","title":"Standalone","text":"<p>cAdvisor is a static Go binary with no external dependencies. To run it standalone all you should need to do is run it! Note that some data sources may require root privileges. cAdvisor will gracefully degrade its features to those it can expose with the access given.</p> <pre><code>$ sudo cadvisor\n</code></pre> <p>cAdvisor is now running (in the foreground) on <code>http://localhost:8080/</code>.</p>"},{"location":"running/#runtime-options","title":"Runtime Options","text":"<p>cAdvisor has a series of flags that can be used to configure its runtime behavior. More details can be found in runtime options.</p>"},{"location":"runtime_options/","title":"cAdvisor Runtime Options","text":"<p>This document describes a set of runtime flags available in cAdvisor.</p>"},{"location":"runtime_options/#container-labels","title":"Container labels","text":"<ul> <li><code>--store_container_labels=false</code> - do not convert container labels and environment variables into labels on prometheus metrics for each container.</li> <li><code>--whitelisted_container_labels</code> - comma separated list of container labels to be converted to labels on prometheus metrics for each container. <code>store_container_labels</code> must be set to false for this to take effect.</li> </ul>"},{"location":"runtime_options/#container-envs","title":"Container envs","text":"<ul> <li><code>--env_metadata_whitelist</code>: a comma-separated list of environment variable keys that needs to be collected for containers, only support containerd and docker runtime for now.</li> </ul>"},{"location":"runtime_options/#limiting-which-containers-are-monitored","title":"Limiting which containers are monitored","text":"<ul> <li><code>--docker_only=false</code> - do not report raw cgroup metrics, except the root cgroup.</li> <li><code>--raw_cgroup_prefix_whitelist</code> - a comma-separated list of cgroup path prefix that needs to be collected even when <code>--docker_only</code> is specified</li> <li><code>--disable_root_cgroup_stats=false</code> - disable collecting root Cgroup stats.</li> </ul>"},{"location":"runtime_options/#container-hints","title":"Container Hints","text":"<p>Container hints are a way to pass extra information about a container to cAdvisor. In this way cAdvisor can augment the stats it gathers. For more information on the container hints format see its definition. Note that container hints are only used by the raw container driver today.</p> <pre><code>--container_hints=\"/etc/cadvisor/container_hints.json\": location of the container hints file\n</code></pre>"},{"location":"runtime_options/#cpu","title":"CPU","text":"<pre><code>--enable_load_reader=false: Whether to enable cpu load reader\n--max_procs=0: max number of CPUs that can be used simultaneously. Less than 1 for default (number of cores).\n</code></pre>"},{"location":"runtime_options/#debugging-and-logging","title":"Debugging and Logging","text":"<p>cAdvisor-native flags that help in debugging:</p> <pre><code>--log_backtrace_at=\"\": when logging hits line file:N, emit a stack trace\n--log_cadvisor_usage=false: Whether to log the usage of the cAdvisor container\n--version=false: print cAdvisor version and exit\n--profiling=false: Enable profiling via web interface host:port/debug/pprof/\n</code></pre> <p>From glog here are some flags we find useful:</p> <pre><code>--log_dir=\"\": If non-empty, write log files in this directory\n--logtostderr=false: log to standard error instead of files\n--alsologtostderr=false: log to standard error as well as files\n--stderrthreshold=0: logs at or above this threshold go to stderr\n--v=0: log level for V logs\n--vmodule=: comma-separated list of pattern=N settings for file-filtered logging\n</code></pre>"},{"location":"runtime_options/#docker","title":"Docker","text":"<pre><code>--docker=\"unix:///var/run/docker.sock\": docker endpoint (default \"unix:///var/run/docker.sock\")\n--docker_root=\"/var/lib/docker\": DEPRECATED: docker root is read from docker info (this is a fallback, default: /var/lib/docker) (default \"/var/lib/docker\")\n--docker-tls: use TLS to connect to docker\n--docker-tls-cert=\"cert.pem\": client certificate for TLS-connection with docker\n--docker-tls-key=\"key.pem\": private key for TLS-connection with docker\n--docker-tls-ca=\"ca.pem\": trusted CA for TLS-connection with docker\n</code></pre>"},{"location":"runtime_options/#podman","title":"Podman","text":"<pre><code>--podman=\"unix:///var/run/podman/podman.sock\": podman endpoint (default \"unix:///var/run/podman/podman.sock\")\n</code></pre>"},{"location":"runtime_options/#housekeeping","title":"Housekeeping","text":"<p>Housekeeping is the periodic actions cAdvisor takes. During these actions, cAdvisor will gather container stats. These flags control how and when cAdvisor performs housekeeping.</p>"},{"location":"runtime_options/#dynamic-housekeeping","title":"Dynamic Housekeeping","text":"<p>Dynamic housekeeping intervals let cAdvisor vary how often it gathers stats. It does this depending on how active the container is. Turning this off provides predictable housekeeping intervals, but increases the resource usage of cAdvisor.</p> <pre><code>--allow_dynamic_housekeeping=true: Whether to allow the housekeeping interval to be dynamic\n</code></pre>"},{"location":"runtime_options/#housekeeping-intervals","title":"Housekeeping Intervals","text":"<p>Intervals for housekeeping. cAdvisor has two housekeepings: global and per-container.</p> <p>Global housekeeping is a singular housekeeping done once in cAdvisor. This typically does detection of new containers. Today, cAdvisor discovers new containers with kernel events so this global housekeeping is mostly used as backup in the case that there are any missed events.</p> <p>Per-container housekeeping is run once on each container cAdvisor tracks. This typically gets container stats.</p> <pre><code>--global_housekeeping_interval=1m0s: Interval between global housekeepings\n--housekeeping_interval=1s: Interval between container housekeepings\n--max_housekeeping_interval=1m0s: Largest interval to allow between container housekeepings (default 1m0s)\n</code></pre>"},{"location":"runtime_options/#http","title":"HTTP","text":"<p>Specify where cAdvisor listens.</p> <pre><code>--http_auth_file=\"\": HTTP auth file for the web UI\n--http_auth_realm=\"localhost\": HTTP auth realm for the web UI (default \"localhost\")\n--http_digest_file=\"\": HTTP digest file for the web UI\n--http_digest_realm=\"localhost\": HTTP digest file for the web UI (default \"localhost\")\n--listen_ip=\"\": IP to listen on, defaults to all IPs\n--port=8080: port to listen (default 8080)\n--url_base_prefix=/: optional path prefix aded to all resource URLs; useful when running cAdvisor behind a proxy. (default /)\n</code></pre>"},{"location":"runtime_options/#local-storage-duration","title":"Local Storage Duration","text":"<p>cAdvisor stores the latest historical data in memory. How long of a history it stores can be configured with the <code>--storage_duration</code> flag.</p> <pre><code>--storage_duration=2m0s: How long to store data.\n</code></pre>"},{"location":"runtime_options/#machine","title":"Machine","text":"<pre><code>--boot_id_file=\"/proc/sys/kernel/random/boot_id\": Comma-separated list of files to check for boot-id. Use the first one that exists. (default \"/proc/sys/kernel/random/boot_id\")\n--machine_id_file=\"/etc/machine-id,/var/lib/dbus/machine-id\": Comma-separated list of files to check for machine-id. Use the first one that exists. (default \"/etc/machine-id,/var/lib/dbus/machine-id\")\n--update_machine_info_interval=5m: Interval between machine info updates. (default 5m)\n</code></pre>"},{"location":"runtime_options/#metrics","title":"Metrics","text":"<pre><code>--application_metrics_count_limit=100: Max number of application metrics to store (per container) (default 100)\n--collector_cert=\"\": Collector's certificate, exposed to endpoints for certificate based authentication.\n--collector_key=\"\": Key for the collector's certificate\n--disable_metrics=&lt;metrics&gt;: comma-separated list of metrics to be disabled. Options are advtcp,app,cpu,cpuLoad,cpu_topology,cpuset,disk,diskIO,hugetlb,memory,memory_numa,network,oom_event,percpu,perf_event,process,referenced_memory,resctrl,sched,tcp,udp. (default advtcp,cpu_topology,cpuset,hugetlb,memory_numa,process,referenced_memory,resctrl,sched,tcp,udp)\n--enable_metrics=&lt;metrics&gt;: comma-separated list of metrics to be enabled. If set, overrides 'disable_metrics'. Options are advtcp,app,cpu,cpuLoad,cpu_topology,cpuset,disk,diskIO,hugetlb,memory,memory_numa,network,oom_event,percpu,perf_event,process,referenced_memory,resctrl,sched,tcp,udp.\n--prometheus_endpoint=\"/metrics\": Endpoint to expose Prometheus metrics on (default \"/metrics\")\n--disable_root_cgroup_stats=false: Disable collecting root Cgroup stats\n</code></pre>"},{"location":"runtime_options/#storage-drivers","title":"Storage Drivers","text":"<pre><code>--storage_driver=\"\": Storage driver to use. Data is always cached shortly in memory, this controls where data is pushed besides the local cache. Empty means none. Options are: &lt;empty&gt;, bigquery, elasticsearch, influxdb, kafka, redis, statsd, stdout\n--storage_driver_buffer_duration=\"1m0s\": Writes in the storage driver will be buffered for this duration, and committed to the non memory backends as a single transaction (default 1m0s)\n--storage_driver_db=\"cadvisor\": database name (default \"cadvisor\")\n--storage_driver_host=\"localhost:8086\": database host:port (default \"localhost:8086\")\n--storage_driver_password=\"root\": database password (default \"root\")\n--storage_driver_secure=false: use secure connection with database\n--storage_driver_table=\"stats\": table name (default \"stats\")\n--storage_driver_user=\"root\": database username (default \"root\")\n</code></pre>"},{"location":"runtime_options/#perf-events","title":"Perf Events","text":"<pre><code>--perf_events_config=\"\" Path to a JSON file containing configuration of perf events to measure. Empty value disables perf events measuring.\n</code></pre> <p>Core perf events can be exposed on Prometheus endpoint per CPU or aggregated by event. It is controlled through <code>--disable_metrics</code> and <code>--enable_metrics</code> parameters with option <code>percpu</code>, e.g.: - <code>--disable_metrics=\"percpu\"</code> - core perf events are aggregated - <code>--disable_metrics=\"\"</code> - core perf events are exposed per CPU.</p> <p>It's possible to get \"too many opened files\" error when a lot of perf events are exposed per CPU. This happens because of passing system limits. Try to increase max number of file desctriptors with <code>ulimit -n &lt;value&gt;</code>.</p> <p>Aggregated form of core perf events significantly decrease volume of data. For aggregated form of core perf events scaling ratio (<code>container_perf_metric_scaling ratio</code>) indicates the lowest value of scaling ratio for specific event to show the worst precision.</p>"},{"location":"runtime_options/#perf-subsystem-introduction","title":"Perf subsystem introduction","text":"<p>One of the goals of kernel perf subsystem is to instrument CPU performance counters that allow to profile applications. Profiling is performed by setting up performance counters that count hardware events (e.g. number of retired instructions, number of cache misses). The counters are CPU hardware registers and amount of them is limited.</p> <p>Other goals of perf subsystem (such as tracing) are beyond the scope of this documentation and you can follow Further Reading section below to learn more about them.</p> <p>Familiarize yourself with following perf-event-related terms: * <code>multiplexing</code> - 2nd Generation Intel\u00ae Xeon\u00ae Scalable Processors provides 4 counters per each hyper thread. If number of configured events is greater than number of available counters then Linux will multiplex counting and some (or even all) of the events will not be accounted for all the time. In such situation information about amount of time that event was accounted for and amount of time when event was enabled is provided. Counter value that cAdvisor exposes is scaled automatically. * <code>grouping</code> - in scenario when accounted for events are used to calculate derivative metrics, it is reasonable to measure them in transactional manner: all the events in a group must be accounted for in the same period of time. Keep in mind that it is impossible to group more events that there are counters available. * <code>uncore events</code> - events which can be counted by PMUs outside core. * <code>PMU</code> - Performance Monitoring Unit</p>"},{"location":"runtime_options/#getting-config-values","title":"Getting config values","text":"<p>Using perf tools: * Identify the event in <code>perf list</code> output. * Execute command: <code>perf stat -I 5000 -vvv -e EVENT_NAME</code> * Find <code>perf_event_attr</code> section on <code>perf stat</code> output, copy config and type field to configuration file.</p> <p><pre><code>------------------------------------------------------------\nperf_event_attr:\n  type                             18\n  size                             112\n  config                           0x304\n  sample_type                      IDENTIFIER\n  read_format                      TOTAL_TIME_ENABLED|TOTAL_TIME_RUNNING\n  disabled                         1\n  inherit                          1\n  exclude_guest                    1\n------------------------------------------------------------\n</code></pre> * Configuration file should look like: <pre><code>{\n  \"core\": {\n    \"events\": [\n      \"event_name\"\n    ],\n    \"custom_events\": [\n      {\n        \"type\": 18,\n        \"config\": [\n          \"0x304\"\n        ],\n        \"name\": \"event_name\"\n      }\n    ]\n  },\n  \"uncore\": {\n    \"events\": [\n      \"event_name\"\n    ],\n    \"custom_events\": [\n      {\n        \"type\": 18,\n        \"config\": [\n          \"0x304\"\n        ],\n        \"name\": \"event_name\"\n      }\n    ]\n  }\n}\n</code></pre></p> <p>Config values can be also obtain from: * Intel\u00ae 64 and IA32 Architectures Performance Monitoring Events</p>"},{"location":"runtime_options/#uncore-events-configuration","title":"Uncore Events configuration","text":"<p>Uncore Event name should be in form <code>PMU_PREFIX/event_name</code> where PMU_PREFIX mean that statistics would be counted on all PMUs with that prefix in name.</p> <p>Let's explain this by example:</p> <pre><code>{\n  \"uncore\": {\n    \"events\": [\n      \"uncore_imc/cas_count_read\",\n      \"uncore_imc_0/cas_count_write\",\n      \"cas_count_all\"\n    ],\n    \"custom_events\": [\n      {\n        \"config\": [\n          \"0x304\"\n        ],\n        \"name\": \"uncore_imc_0/cas_count_write\"\n      },\n      {\n        \"type\": 19,\n        \"config\": [\n          \"0x304\"\n        ],\n        \"name\": \"cas_count_all\"\n      }\n    ]\n  }\n}\n</code></pre> <ul> <li> <p><code>uncore_imc/cas_count_read</code> - because of <code>uncore_imc</code> type and no entry in custom events,     it would be counted by all Integrated Memory Controller PMUs with config provided from libpfm package.     (using this function: https://man7.org/linux/man-pages/man3/pfm_get_os_event_encoding.3.html)</p> </li> <li> <p><code>uncore_imc_0/cas_count_write</code> - because of <code>uncore_imc_0</code> type and entry in custom events it would be counted by <code>uncore_imc_0</code> PMU with provided config.</p> </li> <li> <p><code>uncore_imc_1/cas_count_all</code> - because of entry in custom events with type field, event would be counted by PMU with 19 type and provided config.</p> </li> </ul>"},{"location":"runtime_options/#configuring-perf-events-by-name","title":"Configuring perf events by name","text":"<p>It is possible to configure perf events by names using events supported in libpfm4, for detailed information please see libpfm4 documentation.</p> <p>Discovery of perf events supported on platform can be made using python script - pmu.py provided with libpfm4, please see script reqirements.</p>"},{"location":"runtime_options/#example-configuration-of-perf-events-using-event-names-supported-in-libpfm4","title":"Example configuration of perf events using event names supported in libpfm4","text":"<p>Example output of <code>pmu.py</code>: <pre><code>$ python pmu.py\nINSTRUCTIONS 1\n         u 0\n         k 1\n         period 3\n         freq 4\n         precise 5\n         excl 6\n         mg 7\n         mh 8\n         cpu 9\n         pinned 10\nINSTRUCTION_RETIRED 192\n         e 2\n         i 3\n         c 4\n         t 5\n         intx 7\n         intxcp 8\n         u 0\n         k 1\n         period 3\n         freq 4\n         excl 6\n         mg 7\n         mh 8\n         cpu 9\n         pinned 10\nUNC_M_CAS_COUNT 4\n         RD 3\n         WR 12\n         e 0\n         i 1\n         t 2\n         period 3\n         freq 4\n         excl 6\n         cpu 9\n         pinned 10\n</code></pre> and perf events configuration for listed events: <pre><code>{\n  \"core\": {\n    \"events\": [\n      \"instructions\",\n      \"instruction_retired\"\n    ]\n  },\n  \"uncore\": {\n    \"events\": [\n      \"uncore_imc/unc_m_cas_count:rd\",\n      \"uncore_imc/unc_m_cas_count:wr\"\n    ]\n  }\n}\n</code></pre></p> <p>Notice: PMU_PREFIX is provided in the same way as for configuration with config values.</p>"},{"location":"runtime_options/#grouping","title":"Grouping","text":"<pre><code>{\n  \"core\": {\n    \"events\": [\n      [\"instructions\", \"instruction_retired\"]\n    ]\n  },\n  \"uncore\": {\n    \"events\": [\n      [\"uncore_imc_0/unc_m_cas_count:rd\", \"uncore_imc_0/unc_m_cas_count:wr\"],\n      [\"uncore_imc_1/unc_m_cas_count:rd\", \"uncore_imc_1/unc_m_cas_count:wr\"]\n    ]\n  }\n}\n</code></pre>"},{"location":"runtime_options/#further-reading","title":"Further reading","text":"<ul> <li>perf Examples on Brendan Gregg's blog</li> <li>Kernel Perf Wiki</li> <li><code>man perf_event_open</code></li> <li>perf subsystem in Linux kernel</li> <li>Uncore Performance Monitoring Reference Manuals</li> </ul> <p>See example configuration below: <pre><code>{\n  \"core\": {\n    \"events\": [\n      \"instructions\",\n      \"instructions_retired\"\n    ],\n    \"custom_events\": [\n      {\n        \"type\": 4,\n        \"config\": [\n          \"0x5300c0\"\n        ],\n        \"name\": \"instructions_retired\"\n      }\n    ]\n  },\n  \"uncore\": {\n    \"events\": [\n      \"uncore_imc/cas_count_read\"\n    ],\n    \"custom_events\": [\n      {\n        \"config\": [\n          \"0xc04\"\n        ],\n        \"name\": \"uncore_imc/cas_count_read\"\n      }\n    ]\n  }\n}\n</code></pre></p> <p>In the example above: * <code>instructions</code> will be measured as a non-grouped event and is specified using human friendly interface that can be obtained by calling <code>perf list</code>. You can use any name that appears in the output of <code>perf list</code> command. This is interface that majority of users will rely on. * <code>instructions_retired</code> will be measured as non-grouped event and is specified using an advanced API that allows to specify any perf event available (some of them are not named and can't be specified with plain string). Event name should be a human readable string that will become a metric name. * <code>cas_count_read</code> will be measured as uncore non-grouped event on all Integrated Memory Controllers Performance Monitoring Units because of unset <code>type</code> field and <code>uncore_imc</code> prefix.</p>"},{"location":"runtime_options/#resctrl","title":"Resctrl","text":"<p>To gain metrics, cAdvisor creates own monitoring groups with <code>cadvisor</code> prefix.</p> <p>Resctrl file system is not hierarchical like cgroups, so users should set <code>--docker_only</code> flag to avoid race conditions and unexpected behaviours.</p> <pre><code>--resctrl_interval=0: Resctrl mon groups updating interval. Zero value disables updating mon groups.\n</code></pre>"},{"location":"runtime_options/#storage-driver-specific-instructions","title":"Storage driver specific instructions:","text":"<ul> <li>InfluxDB instructions.</li> <li>ElasticSearch instructions.</li> <li>Kafka instructions.</li> <li>Prometheus instructions.</li> </ul>"},{"location":"web/","title":"cAdvisor Web UI","text":"<p>cAdvisor exposes a web UI at its port:</p> <p><code>http://&lt;hostname&gt;:&lt;port&gt;/</code></p> <p>This UI has one primary resource at <code>/containers</code> which exports live information about all containers on the machine.</p>"},{"location":"web/#web-ui-authentication","title":"Web UI authentication","text":"<p>You can add authentication to the web UI by either HTTP basic or HTTP digest authentication. </p> <p>NOTE: The Web UI authentication only protects the <code>/containers</code> endpoint, and not the other cAdvisor HTTP endpoints such as <code>/api/...</code> and <code>/metrics</code>. Some of these endpoints can expose sensitive information, so it is not advised to expose these endpoints publicly.</p>"},{"location":"web/#http-basic-authentication","title":"HTTP basic authentication","text":"<p>You will need to add a http_auth_file parameter with a HTTP basic auth file generated using htpasswd to enable HTTP basic auth. By default the auth realm is set as localhost.</p> <p><code>./cadvisor --http_auth_file test.htpasswd --http_auth_realm localhost</code></p> <p>The test.htpasswd file provided has a username and password already added (<code>admin:password1</code>) for testing purposes.</p>"},{"location":"web/#http-digest-authentication","title":"HTTP Digest authentication","text":"<p>You will need to add a http_digest_file parameter with a HTTP digest auth file generated using htdigest to enable HTTP Digest auth. By default the auth realm is set as localhost.</p> <p><code>./cadvisor --http_digest_file test.htdigest --http_digest_realm localhost</code></p> <p>The test.htdigest file provided has a username and password already added (<code>admin:password1</code>) for testing purposes.</p> <p>Note : You can use either type of authentication, in case you decide to use both files in the arguments only HTTP basic auth will be enabled. </p>"},{"location":"development/","title":"Development","text":"<p>This part holds documentation for cAdvisor developers and contributors. If you are looking for development using cAdvisor (as opposed to development of cAdvisor), then these documents probably don't apply to you.</p>"},{"location":"development/build/","title":"Building and Testing cAdvisor","text":"<p>Note: cAdvisor only builds on Linux since it uses Linux-only APIs.</p>"},{"location":"development/build/#installing-dependencies","title":"Installing Dependencies","text":"<p>cAdvisor is written in the Go programming language. If you haven't set up a Go development environment, please follow these instructions to install go tool and set up GOPATH. Note that the version of Go in package repositories of some operating systems is outdated, so please download the latest version.</p> <p>Note: cAdvisor requires Go 1.14 to build.</p> <p>After setting up Go, you should be able to <code>go get</code> cAdvisor as expected (we use <code>-d</code> to only download):</p> <pre><code>$ go get -d github.com/google/cadvisor\n</code></pre>"},{"location":"development/build/#building-from-source","title":"Building from Source","text":"<p>At this point you can build cAdvisor from the source folder:</p> <pre><code>$GOPATH/src/github.com/google/cadvisor $ make build\n</code></pre> <p>or run only unit tests:</p> <pre><code>$GOPATH/src/github.com/google/cadvisor $ make test\n</code></pre> <p>For integration tests, see the integration testing page.</p>"},{"location":"development/build/#non-volatile-memory-support","title":"Non-volatile Memory Support","text":"<p>cAdvisor can be linked against libipmctl library that allows to gather information about Intel\u00ae Optane\u2122 DC Persistent memory. If you want to build cAdvisor with libipmctl support you must meet following requirements: * <code>libipmctl-devel</code> must be installed on build system. * <code>libipmctl</code> must be installed on all systems where cAdvisor is running.</p> <p>Detailed information about building <code>libipmctl</code> can be found in the project's README. Make sure to use the most up to date released version. Functionality that relies on <code>libipmctl</code> was tested against version 02.00.00.3820 of the library.</p> <p>To enable <code>libipmctl</code> support <code>GO_FLAGS</code> variable must be set:</p> <pre><code>$GOPATH/src/github.com/google/cadvisor $ GO_FLAGS=\"-tags=libipmctl,netgo\" make build\n</code></pre>"},{"location":"development/build/#perf-support","title":"Perf Support","text":"<p>cAdvisor can be linked against libpfm4 library that allows to gather information about performance monitoring events. If you want to build cAdvisor with libpfm4 support you must meet following requirements: * <code>libpfm4-dev</code> must be installed on build system. * <code>libpfm4</code> must be installed on all systems where cAdvisor is running.</p> <p>libpfm4 packages are available in Debian- and RHEL-derivatives distributions.</p> <p>libpfm4 can be installed using apt package manager: <pre><code>apt-get install libpfm4 libpfm4-dev\n</code></pre> or yum package manager: <pre><code>yum install libpfm libpfm-devel\n</code></pre></p> <p>To enable <code>libpfm4</code> support <code>GO_FLAGS</code> variable must be set:</p> <pre><code>$GOPATH/src/github.com/google/cadvisor $ GO_FLAGS=\"-tags=libpfm,netgo\" make build\n</code></pre>"},{"location":"development/build/#running-built-binary","title":"Running Built Binary","text":"<p>Now you can run the built binary:</p> <pre><code>$GOPATH/src/github.com/google/cadvisor $ sudo ./cadvisor\n</code></pre>"},{"location":"development/build/#perf-support_1","title":"Perf Support","text":"<p>It is required to include perf config (examplary config is available here) to run cAdvisor with performance monitoring events: <pre><code>$GOPATH/src/github.com/google/cadvisor $ sudo ./cadvisor -perf_events_config=perf/testing/perf-non-hardware.json\n</code></pre></p>"},{"location":"development/integration_testing/","title":"Integration Testing cAdvisor","text":""},{"location":"development/integration_testing/#docker-based-tests","title":"Docker-based tests","text":"<p>The cAdvisor integration tests are run per-pr using Github Actions. Workflow configuration can be found at .github/workflows/test.yml. Tests are executed in Docker containers run on MS Azure virtual machines.</p> <p>To run them locally Docker must be installed on your machine. Following command allows you to execute default suite of integration tests:</p> <pre><code>make docker-test-integration\n</code></pre> <p>Build scripts take care of building cAdvisor and integration tests, and executing them against running cAdvisor process.</p> <p>In order to run non-default tests suites (e.g. such that rely on third-party C libraries) you must source one of the files available at build/config, e.g.:</p> <pre><code>source build/config/libpfm4.sh &amp;&amp; make docker-test-integration\n</code></pre> <p>All the necessary packages will be installed, build flags will be applied and additional parameters will be passed to cAdvisor automatically. Configuration is performed using shell environment variables.</p>"},{"location":"development/integration_testing/#vm-base-tests-legacy","title":"VM-base tests (legacy)","text":"<p>The cAdvisor integration tests are run per-pr using the kubernetes node-e2e testing framework on GCE instances.  To make use of this framework, complete the setup of GCP described in the node-e2e testing framework, clone <code>k8s.io/kubernetes</code>, and from that repository run: <pre><code>$ make test-e2e-node TEST_SUITE=cadvisor REMOTE=true\n</code></pre> This will create a VM, build cadvisor, run integration tests on that VM, retrieve logs, and will clean up the test afterwards.  See the node-e2e testing documentation for more running options.</p> <p>To simply run the tests against an existing cAdvisor:</p> <pre><code>$ go test github.com/google/cadvisor/integration/tests/... -host=HOST -port=PORT\n</code></pre> <p>Note that <code>HOST</code> and <code>PORT</code> default to <code>localhost</code> and <code>8080</code> respectively. Today We only support remote execution in Google Compute Engine since that is where we run our continuous builds.</p>"},{"location":"development/issues/","title":"GitHub Issue tracking cAdvisor","text":"<p>This document outlines the process around GitHub issue tracking for cAdvisor at https://github.com/google/cadvisor/issues</p>"},{"location":"development/issues/#labels","title":"Labels","text":"<p>A brief explanation of what issue labels mean. Most labels also apply to pull requests, but for pull requests which reference an issue, it is not necessary to copy the same labels to the PR.</p> <ul> <li><code>area/API</code> - For issues related to the API.</li> <li><code>area/UI</code> - For issues related to the web UI.</li> <li><code>area/documentation</code> - For issues related to the documentation (inline comments or markdown).</li> <li><code>area/performance</code> - For issues related to cAdvisor performance (speed, memory, etc.).</li> <li><code>area/storage</code> - For issues related to cAdvisor storage plugins.</li> <li><code>area/testing</code> - For issues related to testing (integration tests, unit tests, jenkins, etc.)</li> <li><code>closed/duplicate</code> - For issues which have been closed as duplicates of another issue. The final   comment on the issue should hold a reference the duplicate issue.</li> <li><code>closed/infeasible</code> - For issues which cannot be resolved (e.g. a request for a feature we cannot   or do not want to add).</li> <li><code>community-assigned</code> - For issues which are being worked on by a community member (when github won't let us assign the issue to them).</li> <li><code>kind/bug</code> - For issues referring to a bug in the existing implementation.</li> <li><code>kind/enhancement</code> - For issues proposing an enhancement or new feature.</li> <li><code>kind/support</code> - For issues which might just be user confusion / environment setup. If support   issue ends up requiring a PR, it should probably be relabeled (for example, to <code>bug</code>). Many   support issues may indicate a shortcoming of the documentation.</li> <li><code>help wanted</code> - For issues which have been highlighted as a good place to contribute to   cAdvisor. <code>help wanted</code> issues could be enhancements that the core team is unlikely to get to in   the near future, or small projects which might be a good starting point. Lack of a <code>help wanted</code>   label does not mean we won't accept contributions, it only means it was not identified as a   candidate project for community contributions.</li> </ul>"},{"location":"development/releasing/","title":"cAdvisor Release Instructions","text":""},{"location":"development/releasing/#1-send-release-pr","title":"1. Send Release PR","text":"<p>Example: https://github.com/google/cadvisor/pull/1281</p> <p>Add release notes to CHANGELOG.md</p> <ul> <li>Tip: Use a github PR search to find changes since the last release   <code>is:pr is:merged merged:&gt;2016-04-21</code></li> </ul>"},{"location":"development/releasing/#2-create-the-release-tag","title":"2. Create the release tag","text":""},{"location":"development/releasing/#2a-create-the-release-branch-only-for-majorminor-releases","title":"2.a Create the release branch (only for major/minor releases)","text":"<p>Skip this step for patch releases.</p> <pre><code># Example version\nVERSION=v0.23\nPATCH_VERSION=$VERSION.0\n# Sync to HEAD, or the commit to branch at\ngit fetch upstream &amp;&amp; git checkout upstream/master\n# Create the branch\ngit branch release-$VERSION\n# Push it to upstream\ngit push git@github.com:google/cadvisor.git release-$VERSION\n</code></pre>"},{"location":"development/releasing/#2b-tag-the-release-for-all-releases","title":"2.b Tag the release (for all releases)","text":"<pre><code># Example patch version\nVERSION=v0.23\nPATCH_VERSION=$VERSION.0\n# Checkout the release branch\ngit fetch upstream &amp;&amp; git checkout upstream/release-$VERSION\n# Tag the release commit. If you aren't signing, ommit the -s\ngit tag -s -a $PATCH_VERSION\n# Push it to upstream\ngit push git@github.com:google/cadvisor.git $PATCH_VERSION\n</code></pre>"},{"location":"development/releasing/#3-build-release-artifacts","title":"3. Build release artifacts","text":"<p>Command: <code>make release</code></p> <ul> <li>Make sure your git client is synced to the release cut point</li> <li>Use the same go version as kubernetes: dependencies.yaml</li> <li>Tip: use https://github.com/moovweb/gvm to manage multiple go versions.</li> <li>Try to build it from the release branch, since we include that in the binary version</li> <li>Verify the ldflags output, in particular check the Version, BuildUser, and GoVersion are expected</li> </ul> <p>Once the build is complete, copy the output after <code>Release info...</code> and save it to use in step 5</p> <p>Example:</p> <pre><code>Multi Arch Container:\ngcr.io/cadvisor/cadvisor:v0.44.1-test-8\n\nArchitecture Specific Containers:\ngcr.io/cadvisor/cadvisor-arm:v0.44.1-test-8\ngcr.io/cadvisor/cadvisor-arm64:v0.44.1-test-8\ngcr.io/cadvisor/cadvisor-amd64:v0.44.1-test-8\n\nBinaries:\nSHA256 (cadvisor-v0.44.1-test-8-linux-arm64) = e5e3f9e72208bc6a5ef8b837473f6c12877ace946e6f180bce8d81edadf66767\nSHA256 (cadvisor-v0.44.1-test-8-linux-arm) = 7d714e495a4f50d9cc374bd5e6b5c6922ffa40ff1cc7244f2308f7d351c4ccea\nSHA256 (cadvisor-v0.44.1-test-8-linux-amd64) = ea95c5a6db8eecb47379715c0ca260a8a8d1522971fd3736f80006c7f6cc9466\n</code></pre>"},{"location":"development/releasing/#4-check-that-the-containers-for-the-release-work","title":"4. Check that the Containers for the release work","text":"<p>The only argument to the script is the tag of the Multi Arch Container from step 3. To verify that the container images for the release were built successfully, use the check_container.sh script. The script will start each cadvisor image and curl the <code>/healthz</code> endpoint to confirm that it is working.</p> <p>Running this script requires that you have installed <code>qemu-user-static</code> and configured qemu as a binary interpreter.</p> <pre><code>$ sudo apt install qemu-user-static\n$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n</code></pre> <p>The only argument to the script is the tag of the Multi Arch Container from step 3.</p> <pre><code>build/check_container.sh gcr.io/tstapler-gke-dev/cadvisor:v0.44.1-test-8\n</code></pre>"},{"location":"development/releasing/#5-cut-the-release","title":"5. Cut the release","text":"<p>Go to https://github.com/google/cadvisor/releases and click \"Draft a new release\"</p> <ul> <li>\"Tag version\" and \"Release title\" should be preceded by 'v' and then the   version. Select the tag pushed in step 2.b</li> <li>Copy an old release as a template (e.g.   github.com/google/cadvisor/releases/tag/v0.23.1)</li> <li>Body should start with release notes (from CHANGELOG.md)</li> <li>Next are the docker images and binary hashes you copied (from step 3).</li> <li>Upload the binaries build in step 3, they are located in the <code>_output</code>   directory.</li> <li>If this is a minor version release, mark the release as a \"pre-release\"</li> <li>Click publish when done</li> </ul>"},{"location":"storage/","title":"cAdvisor Storage Plugins","text":"<p>cAdvisor supports exporting stats to various storage driver plugins. To enable a storage driver, set the <code>-storage_driver</code> flag.</p>"},{"location":"storage/#storage-drivers","title":"Storage drivers","text":"<ul> <li>BigQuery. See the documentation for usage.</li> <li>ElasticSearch. See the documentation for usage and examples.</li> <li>InfluxDB. See the documentation for usage and examples.</li> <li>Kafka. See the documentation for usage.</li> <li>Prometheus. See the documentation for usage and examples.</li> <li>Redis</li> <li>StatsD. See the documentation for usage and examples.</li> <li><code>stdout</code> - write stats to standard output.</li> </ul>"},{"location":"storage/elasticsearch/","title":"Exporting cAdvisor Stats to ElasticSearch","text":"<p>cAdvisor supports exporting stats to ElasticSearch. To use ES, you need to provide the additional flags to cAdvisor:</p> <p>Set the storage driver as ES:</p> <pre><code> -storage_driver=elasticsearch\n</code></pre> <p>Specify ES host address:</p> <pre><code> -storage_driver_es_host=\"http://elasticsearch:9200\"\n</code></pre> <p>There are also optional flags:</p> <pre><code> # ElasticSearch type name. By default it's \"stats\".\n -storage_driver_es_type=\"stats\"\n # ElasticSearch can use a sniffing process to find all nodes of your cluster automatically. False by default.\n -storage_driver_es_enable_sniffer=false\n</code></pre>"},{"location":"storage/elasticsearch/#examples","title":"Examples","text":"<p>For a detailed tutorial, see docker-elk-cadvisor-dashboards</p>"},{"location":"storage/influxdb/","title":"Exporting cAdvisor Stats to InfluxDB","text":"<p>cAdvisor supports exporting stats to InfluxDB. To use InfluxDB, you need to pass some additional flags to cAdvisor telling it where the InfluxDB instance is located:</p> <p>Set the storage driver as InfluxDB.</p> <pre><code> -storage_driver=influxdb\n</code></pre> <p>Specify what InfluxDB instance to push data to:</p> <pre><code> # The *ip:port* of the database. Default is 'localhost:8086'\n -storage_driver_host=ip:port\n # database name. Uses db 'cadvisor' by default\n -storage_driver_db\n # database username. Default is 'root'\n -storage_driver_user\n # database password. Default is 'root'\n -storage_driver_password\n # Use secure connection with database. False by default\n -storage_driver_secure\n # Writes will be buffered for this duration, and committed to the non memory backends as a single transaction. Default is '60s'\n -storage_driver_buffer_duration\n # retention policy. Default is '' which corresponds to the default retention policy of the influxdb database\n-storage_driver_influxdb_retention_policy\n</code></pre>"},{"location":"storage/influxdb/#examples","title":"Examples","text":"<p>Brian Christner wrote a detailed post on setting up Docker monitoring with cAdvisor and Influxdb.  A docker compose configuration for setting up cadvisor-influxdb-grafana can be found here.</p>"},{"location":"storage/kafka/","title":"Exporting cAdvisor Stats to Kafka","text":"<p>cAdvisor supports exporting stats to Kafka. To use Kafka, you need to provide the additional flags to cAdvisor:</p> <p>Set the storage driver as Kafka:</p> <pre><code> -storage_driver=kafka\n</code></pre> <p>If no broker are provided it will default to a broker listening at localhost:9092, with 'stats' as the default topic.</p> <p>Specify a Kafka broker address:</p> <pre><code>-storage_driver_kafka_broker_list=localhost:9092\n</code></pre> <p>Specify a Kafka topic:</p> <pre><code>-storage_driver_kafka_topic=myTopic\n</code></pre> <p>As of version 9.0. Kafka supports TLS client auth:</p> <pre><code> # To enable TLS client auth support you need to provide the following:\n\n # Location to Certificate Authority certificate\n  -storage_driver_kafka_ssl_ca=/path/to/ca.pem\n\n # Location to client certificate certificate\n  -storage_driver_kafka_ssl_cert=/path/to/client_cert.pem\n\n # Location to client certificate key\n  -storage_driver_kafka_ssl_key=/path/to/client_key.pem\n\n # Verify SSL certificate chain (default: true)\n  -storage_driver_kafka_ssl_verify=false\n</code></pre>"},{"location":"storage/prometheus/","title":"Monitoring cAdvisor with Prometheus","text":"<p>cAdvisor exposes container and hardware statistics as Prometheus metrics out of the box. By default, these metrics are served under the <code>/metrics</code> HTTP endpoint. This endpoint may be customized by setting the <code>-prometheus_endpoint</code> and <code>-disable_metrics</code> or <code>-enable_metrics</code> command-line flags.</p> <p>To collect some of metrics it is required to build cAdvisor with additional flags, for details see build instructions, additional flags are indicated in \"additional build flag\" column in table below.</p> <p>To monitor cAdvisor with Prometheus, simply configure one or more jobs in Prometheus which scrape the relevant cAdvisor processes at that metrics endpoint. For details, see Prometheus's Configuration documentation, as well as the Getting started guide.</p>"},{"location":"storage/prometheus/#examples","title":"Examples","text":"<ul> <li> <p>CenturyLink Labs did an excellent write up on Monitoring Docker services with Prometheus +cAdvisor, while it is great to get a better overview of cAdvisor integration with Prometheus, the PromDash GUI part is outdated as it has been deprecated for Grafana.</p> </li> <li> <p>vegasbrianc provides a starter project for cAdvisor and Prometheus monitoring, alongide a ready-to-use Grafana dashboard.</p> </li> </ul>"},{"location":"storage/prometheus/#prometheus-container-metrics","title":"Prometheus container metrics","text":"<p>The table below lists the Prometheus container metrics exposed by cAdvisor (in alphabetical order by metric name) and corresponding <code>-disable_metrics</code> / <code>-enable_metrics</code> option parameter:</p> Metric name Type Description Unit (where applicable) option parameter additional build flag <code>container_blkio_device_usage_total</code> Counter Blkio device bytes usage bytes diskIO <code>container_cpu_cfs_periods_total</code> Counter Number of elapsed enforcement period intervals cpu <code>container_cpu_cfs_throttled_periods_total</code> Counter Number of throttled period intervals cpu <code>container_cpu_cfs_throttled_seconds_total</code> Counter Total time duration the container has been throttled seconds cpu <code>container_cpu_load_average_10s</code> Gauge Value of container cpu load average over the last 10 seconds cpuLoad <code>container_cpu_schedstat_run_periods_total</code> Counter Number of times processes of the cgroup have run on the cpu sched <code>container_cpu_schedstat_runqueue_seconds_total</code> Counter Time duration processes of the container have been waiting on a runqueue seconds sched <code>container_cpu_schedstat_run_seconds_total</code> Counter Time duration the processes of the container have run on the CPU seconds sched <code>container_cpu_system_seconds_total</code> Counter Cumulative system cpu time consumed seconds cpu <code>container_cpu_usage_seconds_total</code> Counter Cumulative cpu time consumed seconds cpu <code>container_cpu_user_seconds_total</code> Counter Cumulative user cpu time consumed seconds cpu <code>container_file_descriptors</code> Gauge Number of open file descriptors for the container process <code>container_fs_inodes_free</code> Gauge Number of available Inodes disk <code>container_fs_inodes_total</code> Gauge Total number of Inodes disk <code>container_fs_io_current</code> Gauge Number of I/Os currently in progress diskIO <code>container_fs_io_time_seconds_total</code> Counter Cumulative count of seconds spent doing I/Os seconds diskIO <code>container_fs_io_time_weighted_seconds_total</code> Counter Cumulative weighted I/O time seconds diskIO <code>container_fs_limit_bytes</code> Gauge Number of bytes that can be consumed by the container on this filesystem bytes disk <code>container_fs_reads_bytes_total</code> Counter Cumulative count of bytes read bytes diskIO <code>container_fs_read_seconds_total</code> Counter Cumulative count of seconds spent reading diskIO <code>container_fs_reads_merged_total</code> Counter Cumulative count of reads merged diskIO <code>container_fs_reads_total</code> Counter Cumulative count of reads completed diskIO <code>container_fs_sector_reads_total</code> Counter Cumulative count of sector reads completed diskIO <code>container_fs_sector_writes_total</code> Counter Cumulative count of sector writes completed diskIO <code>container_fs_usage_bytes</code> Gauge Number of bytes that are consumed by the container on this filesystem bytes disk <code>container_fs_writes_bytes_total</code> Counter Cumulative count of bytes written bytes diskIO <code>container_fs_write_seconds_total</code> Counter Cumulative count of seconds spent writing seconds diskIO <code>container_fs_writes_merged_total</code> Counter Cumulative count of writes merged diskIO <code>container_fs_writes_total</code> Counter Cumulative count of writes completed diskIO <code>container_hugetlb_failcnt</code> Counter Number of hugepage usage hits limits hugetlb <code>container_hugetlb_max_usage_bytes</code> Gauge Maximum hugepage usages recorded bytes hugetlb <code>container_hugetlb_usage_bytes</code> Gauge Current hugepage usage bytes hugetlb <code>container_last_seen</code> Gauge Last time a container was seen by the exporter timestamp - <code>container_llc_occupancy_bytes</code> Gauge Last level cache usage statistics for container counted with RDT Memory Bandwidth Monitoring (MBM). bytes resctrl <code>container_memory_bandwidth_bytes</code> Gauge Total memory bandwidth usage statistics for container counted with RDT Memory Bandwidth Monitoring (MBM). bytes resctrl <code>container_memory_bandwidth_local_bytes</code> Gauge Local memory bandwidth usage statistics for container counted with RDT Memory Bandwidth Monitoring (MBM). bytes resctrl <code>container_memory_cache</code> Gauge Total page cache memory bytes memory <code>container_memory_failcnt</code> Counter Number of memory usage hits limits memory <code>container_memory_failures_total</code> Counter Cumulative count of memory allocation failures memory <code>container_memory_mapped_file</code> Gauge Size of memory mapped files bytes memory <code>container_memory_max_usage_bytes</code> Gauge Maximum memory usage recorded bytes memory <code>container_memory_migrate</code> Gauge Memory migrate status cpuset <code>container_memory_numa_pages</code> Gauge Number of used pages per NUMA node memory_numa <code>container_memory_rss</code> Gauge Size of RSS bytes memory <code>container_memory_swap</code> Gauge Container swap usage bytes memory <code>container_memory_usage_bytes</code> Gauge Current memory usage, including all memory regardless of when it was accessed bytes memory <code>container_memory_working_set_bytes</code> Gauge Current working set bytes memory <code>container_network_advance_tcp_stats_total</code> Gauge advanced tcp connections statistic for container advtcp <code>container_network_receive_bytes_total</code> Counter Cumulative count of bytes received bytes network <code>container_network_receive_errors_total</code> Counter Cumulative count of errors encountered while receiving network <code>container_network_receive_packets_dropped_total</code> Counter Cumulative count of packets dropped while receiving network <code>container_network_receive_packets_total</code> Counter Cumulative count of packets received network <code>container_network_tcp6_usage_total</code> Gauge tcp6 connection usage statistic for container tcp <code>container_network_tcp_usage_total</code> Gauge tcp connection usage statistic for container tcp <code>container_network_transmit_bytes_total</code> Counter Cumulative count of bytes transmitted bytes network <code>container_network_transmit_errors_total</code> Counter Cumulative count of errors encountered while transmitting network <code>container_network_transmit_packets_dropped_total</code> Counter Cumulative count of packets dropped while transmitting network <code>container_network_transmit_packets_total</code> Counter Cumulative count of packets transmitted network <code>container_network_udp6_usage_total</code> Gauge udp6 connection usage statistic for container udp <code>container_network_udp_usage_total</code> Gauge udp connection usage statistic for container udp <code>container_oom_events_total</code> Counter Count of out of memory events observed for the container oom_event <code>container_perf_events_scaling_ratio</code> Gauge Scaling ratio for perf event counter (event can be identified by <code>event</code> label and <code>cpu</code> indicates the core for which event was measured). See perf event configuration. perf_event libpfm <code>container_perf_events_total</code> Counter Scaled counter of perf core event (event can be identified by <code>event</code> label and <code>cpu</code> indicates the core for which event was measured). See perf event configuration. perf_event libpfm <code>container_perf_uncore_events_scaling_ratio</code> Gauge Scaling ratio for perf uncore event counter (event can be identified by <code>event</code> label, <code>pmu</code> and <code>socket</code> lables indicate the PMU and the CPU socket for which event was measured). See perf event configuration. Metric exists only for main cgroup (id=\"/\"). perf_event libpfm <code>container_perf_uncore_events_total</code> Counter Scaled counter of perf uncore event (event can be identified by <code>event</code> label, <code>pmu</code> and <code>socket</code> lables indicate the PMU and the CPU socket for which event was measured). See perf event configuration). Metric exists only for main cgroup (id=\"/\"). perf_event libpfm <code>container_processes</code> Gauge Number of processes running inside the container process <code>container_referenced_bytes</code> Gauge Container referenced bytes during last measurements cycle based on Referenced field in /proc/smaps file, with /proc/PIDs/clear_refs set to 1 after defined number of cycles configured through <code>referenced_reset_interval</code> cAdvisor parameter.Warning: this is intrusive collection because can influence kernel page reclaim policy and add latency. Refer to https://github.com/brendangregg/wss#wsspl-referenced-page-flag for more details. bytes referenced_memory <code>container_sockets</code> Gauge Number of open sockets for the container process <code>container_spec_cpu_period</code> Gauge CPU period of the container - <code>container_spec_cpu_quota</code> Gauge CPU quota of the container - <code>container_spec_cpu_shares</code> Gauge CPU share of the container - <code>container_spec_memory_limit_bytes</code> Gauge Memory limit for the container bytes - <code>container_spec_memory_reservation_limit_bytes</code> Gauge Memory reservation limit for the container bytes <code>container_spec_memory_swap_limit_bytes</code> Gauge Memory swap limit for the container bytes <code>container_start_time_seconds</code> Gauge Start time of the container since unix epoch seconds <code>container_tasks_state</code> Gauge Number of tasks in given state (<code>sleeping</code>, <code>running</code>, <code>stopped</code>, <code>uninterruptible</code>, or <code>ioawaiting</code>) cpuLoad <code>container_threads</code> Gauge Number of threads running inside the container process <code>container_threads_max</code> Gauge Maximum number of threads allowed inside the container process <code>container_ulimits_soft</code> Gauge Soft ulimit values for the container root process. Unlimited if -1, except priority and nice process"},{"location":"storage/prometheus/#prometheus-hardware-metrics","title":"Prometheus hardware metrics","text":"<p>The table below lists the Prometheus hardware metrics exposed by cAdvisor (in alphabetical order by metric name) and corresponding <code>-disable_metrics</code> / <code>-enable_metrics</code> option parameter:</p> Metric name Type Description Unit (where applicable) option parameter additional build flag <code>machine_cpu_cache_capacity_bytes</code> Gauge Cache size in bytes assigned to NUMA node and CPU core bytes cpu_topology <code>machine_cpu_cores</code> Gauge Number of logical CPU cores <code>machine_cpu_physical_cores</code> Gauge Number of physical CPU cores <code>machine_cpu_sockets</code> Gauge Number of CPU sockets <code>machine_dimm_capacity_bytes</code> Gauge Total RAM DIMM capacity (all types memory modules) value labeled by dimm type,information is retrieved from sysfs edac per-DIMM API (/sys/devices/system/edac/mc/) introduced in kernel 3.6 bytes <code>machine_dimm_count</code> Gauge Number of RAM DIMM (all types memory modules) value labeled by dimm type,information is retrieved from sysfs edac per-DIMM API (/sys/devices/system/edac/mc/) introduced in kernel 3.6 <code>machine_memory_bytes</code> Gauge Amount of memory installed on the machine bytes <code>machine_swap_bytes</code> Gauge Amount of swap memory available on the machine bytes <code>machine_node_distance</code> Gauge Distance between NUMA node and target NUMA node cpu_topology <code>machine_node_hugepages_count</code> Gauge Numer of hugepages assigned to NUMA node cpu_topology <code>machine_node_memory_capacity_bytes</code> Gauge Amount of memory assigned to NUMA node bytes cpu_topology <code>machine_nvm_avg_power_budget_watts</code> Gauge NVM power budget watts libipmctl <code>machine_nvm_capacity</code> Gauge NVM capacity value labeled by NVM mode (memory mode or app direct mode) bytes libipmctl <code>machine_thread_siblings_count</code> Gauge Number of CPU thread siblings cpu_topology"},{"location":"storage/statsd/","title":"Exporting cAdvisor Stats to statsd","text":"<p>cAdvisor supports exporting stats to statsd. To use statsd, you need to pass some additional flags to cAdvisor telling it where to find statsd:</p> <p>Set the storage driver as statsd.</p> <pre><code> -storage_driver=statsd\n</code></pre> <p>Specify what statsd instance to push data to:</p> <pre><code> # The *ip:port* of the instance. Default is 'localhost:8086'\n -storage_driver_host=ip:port\n</code></pre>"},{"location":"storage/statsd/#examples","title":"Examples","text":"<p>The easiest way to get up an running is to start the cadvisor binary with the <code>--storage_driver</code> and <code>--storage_driver_host</code> flags.</p> <pre><code>cadvisor --storage_driver=\"statsd\" --storage_driver_host=\"localhost:8125\"\n</code></pre> <p>The default port for statsd is 8125, so this wil start pumping metrics directly to it.</p>"}]}